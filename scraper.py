import requests, json, os, xml.etree.ElementTree as ET
from datetime import datetime, timedelta

GEMINI_KEY = os.getenv("GEMINI_API_KEY")
COMPANIES = ['ç™¾åº¦', 'é˜¿é‡Œ', 'å­—èŠ‚', 'å°çº¢ä¹¦', 'äº¬ä¸œ', 'æ‹¼å¤šå¤š', 'è…¾è®¯', 'Google', 'AI', 'ç¾å›¢', 'ç½‘æ˜“', 'å°ç±³', 'åä¸º', 'å¿«æ‰‹', 'æ»´æ»´', 'è‹¹æœ', 'ç‰¹æ–¯æ‹‰', 'OpenAI']

def fetch_raw():
    items = []
    sources = [
        "https://rsshub.app/36kr/newsflashes",         # 36æ°ªå¿«è®¯
        "https://rsshub.app/ithome/it",                # ITä¹‹å®¶ï¼ˆç¡¬ä»¶/æ–°æœº/æ–°ç‰ˆè½¯ä»¶ï¼‰
        "https://rsshub.app/tech/news/industry",       # è¡Œä¸šåŠ¨æ€
        "https://rsshub.app/huxiu/article",             # è™å—…
        "https://rsshub.app/cls/depth"                 # è´¢è”ç¤¾ï¼ˆè´¢æŠ¥/å…¬å‘Šï¼‰
    ]
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    
    for url in sources:
        try:
            res = requests.get(url, headers=headers, timeout=30)
            if res.status_code != 200: continue
            root = ET.fromstring(res.text)
            for item in root.findall('./channel/item'):
                title = (item.find('title').text or "").strip()
                link = (item.find('link').text or "").strip()
                desc = (item.find('description').text or "")[:300]
                pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ""
                
                # å…³é”®è¯åŒ¹é…ï¼ŒåŒ…å«â€œå‘å¸ƒâ€ã€â€œæ–°äº§å“â€ã€â€œæµ‹è¯•â€ç­‰åŠ¨ä½œè¯æ›´ä½³
                if any(co.lower() in (title + desc).lower() for co in COMPANIES):
                    items.append({
                        "title": title, 
                        "link": link, 
                        "desc": desc,
                        "pub_date": pub_date
                    })
        except: continue
    return items

def ai_process(items):
    if not GEMINI_KEY or not items: return []
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_KEY}"
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªèµ„æ·±è¡Œä¸šåˆ†æå¸ˆã€‚è¯·å¤„ç†ä»¥ä¸‹æ–°é—»ç¢ç‰‡ï¼š
    1. åˆ¤é‡ï¼šåˆå¹¶è¡¨è¾¾åŒä¸€æ ¸å¿ƒäº‹ä»¶çš„å†…å®¹ï¼Œä¿ç•™æœ€æœ‰ä»·å€¼çš„é‚£ä¸ªé“¾æ¥ã€‚
    2. åˆ†ç±»ï¼š[è–ªé…¬èŒçº§ğŸ’°, ç»„ç»‡å˜åŒ–ğŸ¢, ä¸šåŠ¡åŠ¨æ€ğŸ“¡, è´¢æŠ¥ç ”æŠ¥ğŸ“ˆ, å‘å¸ƒä¼šğŸš€, å°é“æ¶ˆæ¯ğŸ¤«]
    3. ç‰¹åˆ«æ³¨æ„ï¼šå¦‚æœæ˜¯æ–°æ‰‹æœºã€æ–°AppåŠŸèƒ½ã€æ–°æ¨¡å‹ï¼Œè¯·å½’ç±»ä¸ºâ€œå‘å¸ƒä¼šğŸš€â€æˆ–â€œä¸šåŠ¡åŠ¨æ€ğŸ“¡â€ã€‚
    è¿”å›ä¸¥æ ¼JSONæ•°ç»„(å­—æ®µ:company, category, content, link, date)ã€‚
    æ•°æ®ï¼š{json.dumps(items[:50], ensure_ascii=False)}
    """
    
    try:
        res = requests.post(url, json={"contents": [{"parts": [{"text": prompt}]}]}, timeout=40)
        text = res.json()['candidates'][0]['content']['parts'][0]['text']
        # æ¸…æ´— AI å¯èƒ½ç”Ÿæˆçš„ Markdown å—
        json_str = text.strip()
        if "```json" in json_str:
            json_str = json_str.split("```json")[1].split("```")[0].strip()
        elif "```" in json_str:
            json_str = json_str.split("```")[1].split("```")[0].strip()
        return json.loads(json_str)
    except Exception as e:
        print(f"AIå¤„ç†å‡ºé”™: {e}")
        return []

if __name__ == "__main__":
    raw = fetch_raw()
    processed = ai_process(raw)
    
    data_file = 'data.json'
    old_data = []
    if os.path.exists(data_file):
        with open(data_file, 'r', encoding='utf-8') as f:
            try:
                content = f.read()
                old_data = json.loads(content) if content else []
            except: old_data = []

    # é€»è¾‘å»é‡ï¼šä¼˜å…ˆä¿ç•™æ–°æŠ“å–çš„
    combined = processed + old_data
    unique_data = []
    seen_contents = set()
    
    for item in combined:
        # ä½¿ç”¨å†…å®¹å‰ 15 ä¸ªå­—ä½œä¸ºå»é‡æ ‡è¯†
        content_key = item.get('content', '')[:15]
        if content_key not in seen_contents:
            unique_data.append(item)
            seen_contents.add(content_key)
    
    limit_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
    final_list = [i for i in unique_data if i.get('date', '') >= limit_date]
    final_list.sort(key=lambda x: (x.get('date', ''), x.get('company', '')), reverse=True)

    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(final_list, f, ensure_ascii=False, indent=4)
