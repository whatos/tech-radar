import requests, json, os, re
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import xml.etree.ElementTree as ET

GEMINI_KEY = os.getenv("GEMINI_API_KEY")
COMPANIES = ['ç™¾åº¦', 'é˜¿é‡Œ', 'å­—èŠ‚', 'å°çº¢ä¹¦', 'äº¬ä¸œ', 'æ‹¼å¤šå¤š', 'è…¾è®¯', 'Google', 'AI', 'ç¾Žå›¢', 'ç½‘æ˜“', 'å°ç±³', 'åŽä¸º', 'å¿«æ‰‹', 'æ»´æ»´', 'è‹¹æžœ', 'ç‰¹æ–¯æ‹‰', 'OpenAI', 'è‹±ä¼Ÿè¾¾']

def get_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Mobile/15E148 Safari/604.1',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
    }

def fetch_non_rss_sina():
    """ç›´æŽ¥æŠ“å–æ–°æµªç§‘æŠ€æ»šåŠ¨æ–°é—»ç½‘é¡µç‰ˆ (éžRSS)"""
    items = []
    url = "https://tech.sina.com.cn/roll/rollnews.shtml"
    try:
        res = requests.get(url, headers=get_headers(), timeout=20)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        # è§£æžæ–°æµªæ»šåŠ¨æ–°é—»çš„åˆ—è¡¨ç»“æž„
        links = soup.select('.list_005 li a')
        for a in links:
            title = a.text.strip()
            link = a.get('href', '')
            if any(co.lower() in title.lower() for co in COMPANIES):
                items.append({"title": title, "link": link, "desc": "æ¥è‡ªæ–°æµªç§‘æŠ€ç½‘é¡µæŠ“å–"})
    except Exception as e:
        print(f"ç½‘é¡µæŠ“å–å¤±è´¥: {e}")
    return items

def fetch_rss_sources():
    """åŽŸæœ‰çš„ RSS æŠ“å–é€»è¾‘ä½œä¸ºç¨³å®šæ”¯æ’‘"""
    items = []
    sources = [
        "https://rsshub.app/36kr/newsflashes", 
        "https://rsshub.app/ithome/it",
        "https://rsshub.app/cls/depth"
    ]
    for url in sources:
        try:
            res = requests.get(url, headers=get_headers(), timeout=20)
            root = ET.fromstring(res.text)
            for item in root.findall('./channel/item'):
                title = (item.find('title').text or "").strip()
                link = (item.find('link').text or "").strip()
                if any(co.lower() in title.lower() for co in COMPANIES):
                    items.append({"title": title, "link": link, "desc": ""})
        except: continue
    return items

def ai_analyze(items):
    if not GEMINI_KEY or not items: return []
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_KEY}"
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªèµ„æ·±æƒ…æŠ¥åˆ†æžå¸ˆã€‚è¯·å¤„ç†ä»¥ä¸‹ {len(items)} æ¡æ··åˆæ¥æºæ•°æ®ï¼š
    1. ã€åˆå¹¶ã€‘ï¼šè¯­ä¹‰ç›¸åŒçš„æ¡ç›®å¿…é¡»åˆå¹¶ï¼Œä¿ç•™æœ€å…¨çš„ linkã€‚
    2. ã€è´¨é‡ã€‘ï¼šå‰”é™¤çº¯å¹¿å‘Šã€‚
    3. ã€åˆ†ç±»ã€‘ï¼š[è–ªé…¬èŒçº§ðŸ’°, ç»„ç»‡å˜åŒ–ðŸ¢, ä¸šåŠ¡åŠ¨æ€ðŸ“¡, è´¢æŠ¥ç ”æŠ¥ðŸ“ˆ, å‘å¸ƒä¼šðŸš€, å°é“æ¶ˆæ¯ðŸ¤«]
    4. ã€é“¾æŽ¥ã€‘ï¼šlink å¿…é¡»ä¿æŒå®Œæ•´ï¼Œä¸èƒ½ä¿®æ”¹ã€‚
    è¿”å›žä¸¥æ ¼ JSON æ•°ç»„(å­—æ®µ:company, category, content, link, date, score)ã€‚
    æ•°æ®ï¼š{json.dumps(items[:80], ensure_ascii=False)}
    """
    
    try:
        res = requests.post(url, json={"contents": [{"parts": [{"text": prompt}]}]}, timeout=60)
        text = res.json()['candidates'][0]['content']['parts'][0]['text']
        json_str = text.strip().split('```json')[-1].split('```')[0].strip()
        return json.loads(json_str)
    except: return []

if __name__ == "__main__":
    # ç»“åˆ RSS å’Œ ç›´æŽ¥ç½‘é¡µè§£æž
    all_raw = fetch_rss_sources() + fetch_non_rss_sina()
    processed = ai_analyze(all_raw)
    
    data_file = 'data.json'
    old_data = []
    if os.path.exists(data_file):
        with open(data_file, 'r', encoding='utf-8') as f:
            try: old_data = json.load(f)
            except: old_data = []

    # åŽ»é‡åˆå¹¶
    combined = processed + old_data
    unique_data = []
    seen_links = set()
    for i in combined:
        if i.get('link') not in seen_links:
            unique_data.append(i)
            seen_links.add(i.get('link'))
    
    limit_date = (datetime.now() - timedelta(days=10)).strftime("%Y-%m-%d")
    final = [i for i in unique_data if i.get('date', '') >= limit_date]
    final.sort(key=lambda x: (x.get('date', ''), x.get('score', 0)), reverse=True)

    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(final, f, ensure_ascii=False, indent=4)
