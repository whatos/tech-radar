import requests
import json
import os
from datetime import datetime, timedelta

COMPANIES = ['ç™¾åº¦', 'é˜¿é‡Œ', 'å­—èŠ‚', 'å°çº¢ä¹¦', 'äº¬ä¸œ', 'æ‹¼å¤šå¤š', 'è…¾è®¯', 'Google', 'AI', 'ç¾å›¢', 'ç½‘æ˜“']
# æ¢æˆç›´æ¥çš„ API æ¥å£ï¼Œæ›´ç¨³å®š
SOURCES = [
    "https://rsshub.app/36kr/newsflashes",
    "https://rsshub.app/ithome/it"
]

def fetch_new_data():
    news_items = []
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    
    for url in SOURCES:
        try:
            res = requests.get(url, headers=headers, timeout=20)
            from xml.etree import ElementTree as ET
            root = ET.fromstring(res.text)
            for item in root.findall('./channel/item'):
                title = item.find('title').text or ""
                link = item.find('link').text or ""
                matched_co = next((co for co in COMPANIES if co in title), None)
                if matched_co:
                    cat = "ä¸šåŠ¡åŠ¨æ€ğŸ“¡"
                    if any(k in title for k in ["è–ªé…¬", "å·¥èµ„", "è£å‘˜", "å¹´ç»ˆå¥–"]): cat = "è–ªé…¬èŒçº§ğŸ’°"
                    if any(k in title for k in ["æ¶æ„", "ä»»å‘½", "è°ƒæ•´", "å˜åŠ¨"]): cat = "ç»„ç»‡å˜åŒ–ğŸ¢"
                    
                    news_items.append({
                        "id": link,
                        "date": datetime.now().strftime("%Y-%m-%d"),
                        "company": matched_co,
                        "category": cat,
                        "content": title,
                        "link": link
                    })
        except Exception as e:
            print(f"Error fetching {url}: {e}")
            continue
    return news_items

if __name__ == "__main__":
    # 1. å®‰å…¨è¯»å–æ—§æ•°æ®
    old_data = []
    if os.path.exists('data.json'):
        try:
            with open('data.json', 'r', encoding='utf-8') as f:
                old_data = json.load(f)
        except:
            old_data = []

    # 2. æŠ“å–
    new_data = fetch_new_data()
    
    # 3. å¦‚æœæ²¡æŠ“åˆ°æ–°ä¸œè¥¿ï¼Œæ‰‹åŠ¨å¡å…¥ä¸€ä¸ªâ€œç³»ç»ŸçŠ¶æ€â€ï¼Œé˜²æ­¢ data.json ä¸ºç©º
    if not new_data and not old_data:
        new_data.append({
            "id": "status_check",
            "date": datetime.now().strftime("%Y-%m-%d"),
            "company": "ç³»ç»Ÿ",
            "category": "çŠ¶æ€",
            "content": "æ‚Ÿç©ºå“¨å…µå·²ä¸Šçº¿ï¼Œæ­£åœ¨å…¨ç½‘æœç´¢æƒ…æŠ¥ä¸­...",
            "link": "#"
        })

    # 4. åˆå¹¶å»é‡
    existing_ids = {item.get('id') for item in old_data}
    for item in new_data:
        if item['id'] not in existing_ids:
            old_data.append(item)

    # 5. ä¿ç•™ 7 å¤©å¹¶ä¿å­˜
    seven_days_ago = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
    final_data = [item for item in old_data if item.get('date', '') >= seven_days_ago]
    final_data.sort(key=lambda x: x.get('date', ''), reverse=True)

    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=4)
