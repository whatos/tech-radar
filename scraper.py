import requests
import json
from datetime import datetime

# 1. ç›‘æ§ç›®æ ‡ï¼ˆå¤§å‚åå•ï¼‰
COMPANIES = ['ç™¾åº¦', 'é˜¿é‡Œ', 'å­—èŠ‚', 'å°çº¢ä¹¦', 'äº¬ä¸œ', 'æ‹¼å¤šå¤š', 'è…¾è®¯', 'Google', 'AI', 'ç¾å›¢', 'ç½‘æ˜“']

# 2. æ ¸å¿ƒç›‘æ§æºï¼ˆå¢åŠ ç§‘æŠ€ã€æ·±åº¦åˆ†ææºï¼‰
SOURCES = [
    "https://rsshub.app/36kr/newsflashes",     # 36æ°ªå¿«è®¯
    "https://rsshub.app/ithome/it",            # ITä¹‹å®¶
    "https://rsshub.app/latepost/1",           # æ™šç‚¹LatePostï¼ˆæ·±åº¦åˆ†æï¼‰
    "https://rsshub.app/jiemian/v6/news/list?id=1" # ç•Œé¢æ–°é—»ï¼ˆå¤§å‚åŠ¨æ€å¤šï¼‰
]

def fetch_all():
    news_items = []
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    # å…³é”®è¯åº“ï¼šä¸“é—¨ç­›é€‰è–ªé…¬ã€èŒçº§ã€å…«å¦
    PAY_KEYWORDS = ["è–ªé…¬", "å·¥èµ„", "å¹´ç»ˆå¥–", "èŒçº§", "è£å‘˜", "è°ƒè–ª", "base", "æœŸæƒ", "åŒ…", "ç¦åˆ©", "æ™®è°ƒ", "è‚¡ç¥¨"]
    ORG_KEYWORDS = ["æ¶æ„", "å˜åŠ¨", "ä»»å‘½", "è°ƒæ•´", "åˆå¹¶", "æ¢å¸…", "ä¸€æŠŠæ‰‹", "VP", "é«˜ç®¡"]

    for url in SOURCES:
        try:
            res = requests.get(url, headers=headers, timeout=15)
            from xml.etree import ElementTree as ET
            root = ET.fromstring(res.text)
            for item in root.findall('./channel/item'):
                title = item.find('title').text or ""
                link = item.find('link').text or ""
                
                # æ£€æŸ¥å¤§å‚å‘½ä¸­
                matched_co = next((co for co in COMPANIES if co in title), None)
                if matched_co:
                    # ä¼˜å…ˆåˆ¤å®šï¼šè–ªé…¬èŒçº§å…«å¦
                    if any(k in title for k in PAY_KEYWORDS):
                        cat = "è–ªé…¬èŒçº§ğŸ’°"
                    elif any(k in title for k in ORG_KEYWORDS):
                        cat = "ç»„ç»‡å˜åŒ–ğŸ¢"
                    else:
                        cat = "ä¸šåŠ¡åŠ¨æ€ğŸ“¡"
                    
                    news_items.append({
                        "date": datetime.now().strftime("%Y-%m-%d"),
                        "company": matched_co,
                        "category": cat,
                        "content": title,
                        "link": link
                    })
        except:
            continue
    return news_items

if __name__ == "__main__":
    data = fetch_all()
    # æŒ‰ç…§åˆ†ç±»ä¼˜å…ˆçº§æ’åºï¼ˆè–ªé…¬å…«å¦æ’åœ¨æœ€å‰ï¼‰
    priority = {"è–ªé…¬èŒçº§ğŸ’°": 0, "ç»„ç»‡å˜åŒ–ğŸ¢": 1, "ä¸šåŠ¡åŠ¨æ€ğŸ“¡": 2}
    data = sorted(data, key=lambda x: priority.get(x['category'], 3))
    
    # ç¡®ä¿æ–‡ä»¶å†™å…¥
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
