import requests
import json
import os
from datetime import datetime, timedelta
import xml.etree.ElementTree as ET

COMPANIES = ['ç™¾åº¦', 'é˜¿é‡Œ', 'å­—èŠ‚', 'å°çº¢ä¹¦', 'äº¬ä¸œ', 'æ‹¼å¤šå¤š', 'è…¾è®¯', 'Google', 'AI', 'ç¾å›¢', 'ç½‘æ˜“']

def fetch_data():
    news_items = []
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    
    # å¢åŠ æ›´å¤šæºï¼šæ–°æµªç§‘æŠ€ã€36Krã€ITä¹‹å®¶
    rss_urls = [
        "https://rsshub.app/36kr/newsflashes",
        "https://rsshub.app/ithome/it",
        "https://rsshub.app/sina/tech/weibo"
    ]
    
    for url in rss_urls:
        try:
            res = requests.get(url, headers=headers, timeout=15)
            root = ET.fromstring(res.text)
            for item in root.findall('./channel/item'):
                title = item.find('title').text or ""
                link = item.find('link').text or ""
                
                # æ£€æŸ¥å¤§å‚å…³é”®è¯
                matched_co = next((co for co in COMPANIES if co.lower() in title.lower()), None)
                if matched_co:
                    cat = "ä¸šåŠ¡åŠ¨æ€ğŸ“¡"
                    if any(k in title for k in ["è–ªé…¬", "å¹´ç»ˆå¥–", "è£å‘˜", "ç¦åˆ©"]): cat = "è–ªé…¬èŒçº§ğŸ’°"
                    if any(k in title for k in ["æ¶æ„", "å˜åŠ¨", "ä»»å‘½", "è°ƒæ•´"]): cat = "ç»„ç»‡å˜åŒ–ğŸ¢"
                    
                    news_items.append({
                        "id": link,
                        "date": datetime.now().strftime("%Y-%m-%d"),
                        "company": matched_co,
                        "category": cat,
                        "content": title,
                        "link": link
                    })
        except: continue
    return news_items

if __name__ == "__main__":
    # 1. è¯»å–æ—§æ•°æ®
    data_file = 'data.json'
    all_data = []
    if os.path.exists(data_file):
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                all_data = json.load(f)
        except: all_data = []

    # 2. æŠ“å–æ–°æ•°æ®
    new_items = fetch_data()
    
    # 3. åˆå¹¶å¹¶å»é‡
    existing_ids = {item.get('id') for item in all_data}
    for item in new_items:
        if item['id'] not in existing_ids:
            all_data.append(item)

    # 4. ä¿åº•ï¼šå¦‚æœçœŸçš„å…¨ç½‘éƒ½æ²¡å¤§å‚æ–°é—»ï¼ˆæ¦‚ç‡æä½ï¼‰ï¼Œç•™ä¸€ä¸ªç³»ç»Ÿæç¤º
    if not all_data:
        all_data.append({"id":"init","date":datetime.now().strftime("%Y-%m-%d"),"company":"ç³»ç»Ÿ","category":"çŠ¶æ€","content":"å“¨å…µå·²å°±ä½ï¼Œæ­£æŒç»­æ‰«æå…¨ç½‘æƒ…æŠ¥...","link":"#"})

    # 5. åªç•™æœ€è¿‘ 7 å¤©ï¼Œæ’åºå¹¶å†™å…¥
    limit_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
    final_list = [i for i in all_data if i.get('date', '') >= limit_date]
    final_list.sort(key=lambda x: (x.get('date', ''), x.get('id', '')), reverse=True)

    with open(data_file, 'w', encoding
