import requests
import json
import os
from datetime import datetime, timedelta
import xml.etree.ElementTree as ET

COMPANIES = ['ç™¾åº¦', 'é˜¿é‡Œ', 'å­—èŠ‚', 'å°çº¢ä¹¦', 'äº¬ä¸œ', 'æ‹¼å¤šå¤š', 'è…¾è®¯', 'Google', 'AI', 'ç¾å›¢', 'ç½‘æ˜“']

def fetch_data():
    news_items = []
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    
    # æˆ‘ä»¬å‡å°‘ä¸ç¨³å®šçš„æºï¼Œå¢åŠ ä¸€ä¸ªæ›´ç¨³çš„æº
    rss_urls = [
        "https://rsshub.app/36kr/newsflashes",
        "https://rsshub.app/ithome/it"
    ]
    
    for url in rss_urls:
        try:
            # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 30 ç§’ï¼Œé˜²æ­¢ GitHub æŠ¥é”™
            res = requests.get(url, headers=headers, timeout=30)
            if res.status_code != 200: continue
            
            root = ET.fromstring(res.text)
            for item in root.findall('./channel/item'):
                title = item.find('title').text or ""
                link = item.find('link').text or ""
                
                matched_co = next((co for co in COMPANIES if co.lower() in title.lower()), None)
                if matched_co:
                    cat = "ä¸šåŠ¡åŠ¨æ€ğŸ“¡"
                    if any(k in title for k in ["è–ªé…¬", "å¹´ç»ˆå¥–", "è£å‘˜"]): cat = "è–ªé…¬èŒçº§ğŸ’°"
                    if any(k in title for k in ["æ¶æ„", "å˜åŠ¨", "ä»»å‘½"]): cat = "ç»„ç»‡å˜åŒ–ğŸ¢"
                    
                    news_items.append({
                        "id": link,
                        "date": datetime.now().strftime("%Y-%m-%d"),
                        "company": matched_co,
                        "category": cat,
                        "content": title,
                        "link": link
                    })
        except Exception as e:
            print(f"è­¦å‘Š: æŠ“å– {url} å¤±è´¥ï¼ŒåŸå› : {e}")
            continue # ä¸€ä¸ªæºåäº†ï¼Œç»§ç»­è·‘ä¸‹ä¸€ä¸ª
    return news_items

if __name__ == "__main__":
    data_file = 'data.json'
    all_data = []
    
    # 1. è¯»å–æ—§æ•°æ®ï¼ˆå¸¦é”™è¯¯ä¿æŠ¤ï¼‰
    if os.path.exists(data_file):
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if content: all_data = json.loads(content)
        except: all_data = []

    # 2. æŠ“å–
    new_items = fetch_data()
    
    # 3. å»é‡åˆå¹¶
    existing_ids = {item.get('id') for item in all_data if isinstance(item, dict)}
    for item in new_items:
        if item['id'] not in existing_ids:
            all_data.append(item)

    # 4. ä¿åº•æ•°æ®ï¼ˆé˜²æ­¢é¡µé¢ç©ºç™½ï¼‰
    if not all_data:
        all_data = [{"id":"init","date":datetime.now().strftime("%Y-%m-%d"),"company":"ç³»ç»Ÿ","category":"çŠ¶æ€","content":"æ‚Ÿç©ºå“¨å…µå·¡é€»ä¸­ï¼Œæš‚æœªå‘ç°å¤§å‚é‡ç£…å¤´æ¡ã€‚","link":"#"}]

    # 5. åªç•™ 7 å¤©å¹¶ä¿å­˜
    try:
        limit_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
        final_list = [i for i in all_data if isinstance(i, dict) and i.get('date', '') >= limit_date]
        final_list.sort(key=lambda x: x.get('date', ''), reverse=True)

        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(final_list, f, ensure_ascii=False, indent=4)
        print("æ•°æ®æ›´æ–°æˆåŠŸï¼")
    except Exception as e:
        print(f"å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
