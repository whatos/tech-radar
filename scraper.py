import requests, json, os, xml.etree.ElementTree as ET
from datetime import datetime, timedelta

GEMINI_KEY = os.getenv("GEMINI_API_KEY")
COMPANIES = ['ç™¾åº¦', 'é˜¿é‡Œ', 'å­—èŠ‚', 'å°çº¢ä¹¦', 'äº¬ä¸œ', 'æ‹¼å¤šå¤š', 'è…¾è®¯', 'Google', 'AI', 'ç¾Žå›¢', 'ç½‘æ˜“', 'å°ç±³', 'åŽä¸º', 'å¿«æ‰‹', 'æ»´æ»´', 'è‹¹æžœ', 'ç‰¹æ–¯æ‹‰', 'OpenAI']

def fetch_raw():
    items = []
    sources = [
        "https://rsshub.app/36kr/newsflashes", 
        "https://rsshub.app/ithome/it",
        "https://rsshub.app/cls/depth",
        "https://rsshub.app/huxiu/article"
    ]
    headers = {'User-Agent': 'Mozilla/5.0'}
    for url in sources:
        try:
            res = requests.get(url, headers=headers, timeout=30)
            root = ET.fromstring(res.text)
            for item in root.findall('./channel/item'):
                title = item.find('title').text or ""
                link = item.find('link').text or ""
                desc = item.find('description').text or ""
                if any(co.lower() in (title + desc).lower() for co in COMPANIES):
                    items.append({"title": title, "link": link, "desc": desc[:400]})
        except: continue
    return items

def ai_analyze(items):
    if not GEMINI_KEY or not items: return []
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_KEY}"
    
    # å……åˆ†åˆ©ç”¨ AI é€»è¾‘ï¼šåˆ†æžã€æ‘˜è¦ã€åŽ»é‡
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ç§‘æŠ€åª’ä½“ç¼–è¾‘å’Œè¡Œä¸šåˆ†æžå¸ˆã€‚
    è¯·é˜…è¯»ä»¥ä¸‹æ–°é—»æµï¼Œæ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
    1. ã€æ·±åº¦åˆ¤é‡ã€‘ï¼šå°†æŽ¢è®¨åŒä¸€äº‹ä»¶çš„å¤šä¸ªæ¡ç›®åˆå¹¶ï¼Œé€‰å‡ºæœ€å…¨é¢ã€æœ€å®¢è§‚çš„æè¿°ã€‚
    2. ã€æƒ…æŠ¥åŠ å·¥ã€‘ï¼šå°†å†—é•¿çš„æ ‡é¢˜è½¬åŒ–ä¸ºç®€ç»ƒçš„â€œä¸€å¥è¯æƒ…æŠ¥â€ã€‚
    3. ã€ä»·å€¼åˆ†ç±»ã€‘ï¼š[è–ªé…¬èŒçº§ðŸ’°, ç»„ç»‡å˜åŒ–ðŸ¢, ä¸šåŠ¡åŠ¨æ€ðŸ“¡, è´¢æŠ¥ç ”æŠ¥ðŸ“ˆ, å‘å¸ƒä¼šðŸš€, å°é“æ¶ˆæ¯ðŸ¤«]
    4. ã€æƒ…æŠ¥åˆ†çº§ã€‘ï¼šæ ¹æ®é‡è¦ç¨‹åº¦ç»™å‡ºä¸€ä¸ªè¯„åˆ†(1-5åˆ†)ã€‚
    5. ã€è‡ªåŠ¨æº¯æ—¶ã€‘ï¼šè‹¥æ–°é—»ä¸­æåˆ°â€œä»Šæ—¥ã€æ˜¨æ—¥â€ï¼Œè¯·å¯¹åº”åˆ° YYYY-MM-DD æ ¼å¼ã€‚
    è¿”å›žä¸¥æ ¼JSONæ•°ç»„(å­—æ®µ:company, category, content, link, date, score)ã€‚
    å¾…å¤„ç†ï¼š{json.dumps(items[:40], ensure_ascii=False)}
    """
    
    try:
        res = requests.post(url, json={"contents": [{"parts": [{"text": prompt}]}]}, timeout=50)
        text = res.json()['candidates'][0]['content']['parts'][0]['text']
        # æ¸…æ´— Markdown æ ¼å¼
        json_str = text.strip().split('```json')[-1].split('```')[0].strip()
        return json.loads(json_str)
    except: return []

if __name__ == "__main__":
    raw = fetch_raw()
    processed = ai_analyze(raw)
    
    data_file = 'data.json'
    old_data = []
    if os.path.exists(data_file):
        with open(data_file, 'r', encoding='utf-8') as f:
            try: old_data = json.load(f)
            except: old_data = []

    # é€»è¾‘åŽ»é‡
    combined = processed + old_data
    unique_data = []
    seen_keys = set()
    for i in combined:
        key = i.get('content', '')[:12] # è¯­ä¹‰ç›¸ä¼¼åº¦åˆåˆ¤
        if key not in seen_keys:
            unique_data.append(i)
            seen_keys.add(key)
    
    # ä¿æŒæœ€è¿‘7å¤©ä¸”æŒ‰æ—¥æœŸå’Œåˆ†å€¼æŽ’åº
    limit_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
    final = [i for i in unique_data if i.get('date', '') >= limit_date]
    final.sort(key=lambda x: (x.get('date', ''), x.get('score', 0)), reverse=True)

    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(final, f, ensure_ascii=False, indent=4)
