import requests
import json
import os
from datetime import datetime, timedelta

COMPANIES = ['ç™¾åº¦', 'é˜¿é‡Œ', 'å­—èŠ‚', 'å°çº¢ä¹¦', 'äº¬ä¸œ', 'æ‹¼å¤šå¤š', 'è…¾è®¯', 'Google', 'AI', 'ç¾å›¢', 'ç½‘æ˜“']
SOURCES = [
    "https://rsshub.app/36kr/newsflashes",
    "https://rsshub.app/ithome/it",
    "https://rsshub.app/latepost/1",
    "https://rsshub.app/jiemian/v6/news/list?id=1"
]

def fetch_new_data():
    news_items = []
    headers = {'User-Agent': 'Mozilla/5.0'}
    PAY_KEYWORDS = ["è–ªé…¬", "å·¥èµ„", "å¹´ç»ˆå¥–", "èŒçº§", "è£å‘˜", "è°ƒè–ª", "base", "æœŸæƒ", "ç¦åˆ©", "æ™®è°ƒ"]
    ORG_KEYWORDS = ["æ¶æ„", "å˜åŠ¨", "ä»»å‘½", "è°ƒæ•´", "åˆå¹¶", "æ¢å¸…", "é«˜ç®¡"]

    for url in SOURCES:
        try:
            res = requests.get(url, headers=headers, timeout=15)
            from xml.etree import ElementTree as ET
            root = ET.fromstring(res.text)
            for item in root.findall('./channel/item'):
                title = item.find('title').text or ""
                link = item.find('link').text or ""
                matched_co = next((co for co in COMPANIES if co in title), None)
                if matched_co:
                    if any(k in title for k in PAY_KEYWORDS): cat = "è–ªé…¬èŒçº§ğŸ’°"
                    elif any(k in title for k in ORG_KEYWORDS): cat = "ç»„ç»‡å˜åŒ–ğŸ¢"
                    else: cat = "ä¸šåŠ¡åŠ¨æ€ğŸ“¡"
                    
                    news_items.append({
                        "id": link, # ç”¨é“¾æ¥åšå”¯ä¸€æ ‡è¯†é˜²æ­¢é‡å¤
                        "date": datetime.now().strftime("%Y-%m-%d"),
                        "company": matched_co,
                        "category": cat,
                        "content": title,
                        "link": link
                    })
        except: continue
    return news_items

if __name__ == "__main__":
    # 1. è¯»å–ç°æœ‰æ•°æ®
    old_data = []
    if os.path.exists('data.json'):
        with open('data.json', 'r', encoding='utf-8') as f:
            try: old_data = json.load(f)
            except: old_data = []

    # 2. æŠ“å–æ–°æ•°æ®å¹¶å»é‡åˆå¹¶
    new_data = fetch_new_data()
    existing_ids = {item['id'] for item in old_data}
    for item in new_data:
        if item['id'] not in existing_ids:
            old_data.append(item)

    # 3. åªä¿ç•™æœ€è¿‘ 7 å¤©
    seven_days_ago = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
    final_data = [item for item in old_data if item['date'] >= seven_days_ago]

    # 4. æ’åºï¼ˆæ—¥æœŸå€’åºï¼Œæœ€æ–°çš„åœ¨å‰ï¼‰
    final_data.sort(key=lambda x: x['date'], reverse=True)

    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=4)
