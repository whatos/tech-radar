import requests, json, os, xml.etree.ElementTree as ET
from datetime import datetime, timedelta

# ä» GitHub Secrets è¯»å–
GEMINI_KEY = os.getenv("GEMINI_API_KEY")
COMPANIES = ['ç™¾åº¦', 'é˜¿é‡Œ', 'å­—èŠ‚', 'å°çº¢ä¹¦', 'äº¬ä¸œ', 'æ‹¼å¤šå¤š', 'è…¾è®¯', 'Google', 'AI', 'ç¾å›¢', 'ç½‘æ˜“']

def fetch_raw():
    items = []
    headers = {'User-Agent': 'Mozilla/5.0'}
    sources = ["https://rsshub.app/36kr/newsflashes", "https://rsshub.app/ithome/it"]
    for url in sources:
        try:
            res = requests.get(url, headers=headers, timeout=20)
            root = ET.fromstring(res.text)
            for item in root.findall('./channel/item'):
                title = item.find('title').text or ""
                link = item.find('link').text or ""
                if any(co.lower() in title.lower() for co in COMPANIES):
                    items.append({"title": title, "link": link})
        except: continue
    return items

def ai_process(items):
    """å…è´¹ AI æ ¸å¿ƒé€»è¾‘ï¼šè¯­ä¹‰å»é‡ & è‡ªåŠ¨æ‰“æ ‡"""
    if not GEMINI_KEY or not items: return []
    
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_KEY}"
    prompt = f"ä½ æ˜¯ä¸€ä¸ªè¡Œä¸šåˆ†æå¸ˆã€‚è¯·å¤„ç†æ–°é—»ï¼š1.å»é‡è¯­ä¹‰é‡å¤é¡¹ï¼›2.åˆ†ç±»ä¸º[è–ªé…¬èŒçº§ğŸ’°, ç»„ç»‡å˜åŒ–ğŸ¢, ä¸šåŠ¡åŠ¨æ€ğŸ“¡]ï¼›3.æå–å…¬å¸åã€‚ä¸¥æ ¼è¿”å›JSONæ•°ç»„(å­—æ®µ:company, category, content, link)ã€‚æ•°æ®ï¼š{json.dumps(items[:15], ensure_ascii=False)}"
    
    try:
        res = requests.post(url, json={"contents": [{"parts": [{"text": prompt}]}]}, timeout=30)
        text = res.json()['candidates'][0]['content']['parts'][0]['text']
        return json.loads(text.replace('```json', '').replace('```', '').strip())
    except Exception as e:
        print(f"AI å¼‚å¸¸: {e}")
        return []

if __name__ == "__main__":
    raw = fetch_raw()
    processed = ai_process(raw)
    
    # åŠ ä¸Šæ—¥æœŸ
    curr_date = datetime.now().strftime("%Y-%m-%d")
    for item in processed: item['date'] = curr_date
        
    # è¯»å– & åˆå¹¶ & 7å¤©ä¿ç•™
    data_file = 'data.json'
    old_data = []
    if os.path.exists(data_file):
        with open(data_file, 'r', encoding='utf-8') as f:
            try: old_data = json.load(f)
            except: old_data = []

    # ç®€å•å»é‡ï¼šå¦‚æœå†…å®¹å·²å­˜åœ¨åˆ™ä¸æ·»åŠ 
    existing_contents = {i.get('content') for i in old_data}
    new_data = [i for i in processed if i.get('content') not in existing_contents]
    
    final_list = (new_data + old_data)
    limit_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
    final_list = [i for i in final_list if i.get('date', '') >= limit_date]
    final_list.sort(key=lambda x: x.get('date', ''), reverse=True)

    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(final_list, f, ensure_ascii=False, indent=4)
